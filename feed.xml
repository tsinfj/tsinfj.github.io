<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sustcsonglin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sustcsonglin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-08T11:26:57+00:00</updated><id>https://sustcsonglin.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DeltaNet Explained (Part I)</title><link href="https://sustcsonglin.github.io/blog/2024/deltanet-1/" rel="alternate" type="text/html" title="DeltaNet Explained (Part I)"/><published>2024-12-03T22:25:00+00:00</published><updated>2024-12-03T22:25:00+00:00</updated><id>https://sustcsonglin.github.io/blog/2024/deltanet-1</id><content type="html" xml:base="https://sustcsonglin.github.io/blog/2024/deltanet-1/"><![CDATA[<p><strong>This blog post series accompanies our NeurIPS ‚Äò24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a> (w/ <a href="https://berlino.github.io/">Bailin Wang</a>, <a href="https://yzhang.site/">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/">Yoon Kim</a>). You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf">here</a>.</strong></p> <ol> <li><a href="#">Part I - The Model</a></li> <li><a href="/blog/2024/deltanet-2/">Part II - The Algorithm</a></li> <li><a href="/blog/2024/deltanet-3/">Part III - The Neural Architecture</a></li> </ol> <h2 id="linear-attention-as-rnn">Linear attention as RNN</h2> <p>Notations: we use CAPITAL BOLD letters to represent matrices, lowercase bold letters to represent vectors, and regular lowercase letters to represent scalars.</p> <h3 id="what-is-linear-attention">What is linear attention?</h3> <p>The vanilla softmax attention mechanism, though powerful, suffers from quadratic complexity in sequence length. Let‚Äôs see how linear attention addresses this issue by starting with the standard softmax attention (assuming single head):</p> \[\begin{aligned} \mathrm{Parallel\ training:} &amp;&amp;&amp; \mathbf{O} = \mathrm{softmax}(\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M})\mathbf{V} &amp;&amp;\in \mathbb{R}^{L\times d} \\ \mathrm{Iterative\ inference:} &amp;&amp;&amp;\mathbf{o_t} = \sum_{j=1}^t \frac{\exp(\mathbf{q}_t^\top \mathbf{k}_j)}{\sum_{l=1}^t\exp(\mathbf{q}^\top_t \mathbf{k}_l)}\mathbf{v}_j &amp;&amp;\in \mathbb{R}^d \end{aligned}\] <p>Here,</p> <ul> <li> <p>\(L\) represents sequence length</p> </li> <li> <p>\(d\) represents head dimension</p> </li> <li> <p>\(\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O} \in \mathbb{R}^{L \times d}\) represent the query, key, value, and output matrices respectively.</p> </li> <li> <p>\(\mathbf{M} \in \mathbb{R}^{L \times L}\) is the causal mask for autoregressive modeling by ensuring each position can only attend to previous positions.</p> </li> </ul> <p>What linear attention<d-cite key="katharopoulos2020transformers"></d-cite> does is to simply remove the softmax operator <d-footnote>The original linear attention formulation incorporates feature mapping on queries and keys along with a normalizer term, but recent studies suggest these components may not be essential.<d-cite key="mao_fine-tuning_2022"></d-cite><d-cite key="sun2023retentive"></d-cite>.</d-footnote>:</p> \[\begin{aligned} \mathrm{Parallel\ trainingÔºö} &amp;&amp;&amp;\mathbf{O}= (\mathbf{Q}\mathbf{K}^\top \odot \mathbf{M})\mathbf{V} &amp;&amp;\in \mathbb{R}^{L\times d} \\ \mathrm{Iterative\ inferenceÔºö}&amp;&amp;&amp;\mathbf{o_t} = \sum_{j=1}^t (\mathbf{q}_t^\top \mathbf{k}_j) \mathbf{v}_j &amp;&amp;\in \mathbb{R}^d \end{aligned}\] <p>While removing softmax alone doesn‚Äôt immediately reduce computational complexity, it enables a crucial mathematical property: linearity. This property, particularly associativity, allows us to restructure the computations in ways that significantly improve efficiency. For training, researchers have developed <strong>chunkwise parallel</strong> techniques<d-cite key="GAU"></d-cite><d-cite key="sun2023retentive"></d-cite><d-cite key="yang_gated_2023"></d-cite> that leverage this linearity to achieve subquadratic complexity while maintaining hardware efficiency, which forms the foundation of our open-source <strong>flash-linear-attention</strong> library<d-cite key="yang_fla_2024"></d-cite>.</p> <p>For inference, we can also rearrange the computation as follows:</p> <p>\(\begin{aligned} &amp;&amp;&amp;&amp;\mathbf{o_t} = \sum_{j=1}^t \mathbf{v}_j(\mathbf{k}_j^\top \mathbf{q}_t) &amp;&amp;&amp;&amp;&amp; \mathbf{k}_j^\top \mathbf{q}_t = \mathbf{q}_t^\top \mathbf{k}_j \in \mathbb{R}\\ &amp;&amp;&amp;&amp;= (\sum_{j=1}^t\mathbf{v}_j\mathbf{k}_j^\top)\mathbf{q}_t &amp;&amp;&amp;&amp;&amp;\text{By associativity} \end{aligned}\) $$</p> <p>Let‚Äôs define a state matrix \(\mathbf{S}_t = \sum_{j=1}^t\mathbf{v}_j\mathbf{k}_j^\top\). Then the computation can be expressed as:</p> \[\mathbf{S}_t = \mathbf{S}_{t-1} + \mathbf{v}_t\mathbf{k}_t^\top \in \mathbb{R}^{d\times d}, \quad \mathbf{o}_t = \mathbf{S}_t \mathbf{q}_t \in \mathbb{R}^{d}\] <p>This formulation reveals that linear attention is essentially a <strong>linear RNN with a matrix-valued state</strong> \(\mathbf{S}\) that accumulates key-value outer products, enabling efficient state (size) expansion from \(\mathcal{O}(d)\) to \(\mathcal{O}(d^2)\).</p> <details> <summary> Why do we want state expansion?</summary> Traditionally, RNN's hidden dimension is often the same (or of the same magnitude) as the input dimension, due to the expensive matrix-multiply-based state update. However, RNN solely relies on the recurrent state to remember the entire history and state size tends to be the bottleneck to remember sufficient amount of information, especially in retrieval tasks. We've been observing a substantial amount of research investigating hardware-efficient state expansion since Mamba1<d-cite key="Gu2023MambaLS"></d-cite> explicitly pointed it out, and linear attention styled outer-product-based update has proven to be optimal in terms of efficiently scaling state up (Mamba2<d-cite key="mamba2"></d-cite> also adopts this strategy!). In our previous HGRN2 work<d-cite key="qin_hgrn2_2024"></d-cite>, we investigated different approaches for state expansion, and the outer product based mechanism has proven to be both performant and scalable. </details> <p>With this approach, we only need to store and update \(\mathbf{S}_t\) instead of maintaining all previous key-value pairs. This optimization dramatically improves efficiency: the time complexity for autoregressive inference reduces from \(\mathcal{O}(L^2d)\) to \(\mathcal{O}(Ld^2)\), while the space complexity improves from \(\mathcal{O}(Ld)\) to \(\mathcal{O}(d^2)\). These improvements make this method particularly advantageous in two scenarios:</p> <ul> <li> <p><strong>Long sequence modeling</strong> where quadratic complexity of softmax attention could be a significant bottleneck.</p> </li> <li> <p>During <strong>generation</strong>, where computation is usually <strong>memory-bound</strong>, removing the KV cache can significantly enhance <strong>inference latency</strong> for \(L \gg d\).</p> </li> </ul> <h3 id="no-free-lunch-key-limitations-of-linear-attention">No Free Lunch: Key Limitations of Linear Attention</h3> <p>Unfortunately, there is no free lunch. The fixed-size state matrix in linear attention means it cannot perfectly preserve all historical information, making exact retrieval particularly challenging.</p> <p>More formally, linear attention implements a key-value associative memory, which is the sum of outer products between keys and values \(\mathbf{S} = \sum \mathbf{v}_i\mathbf{k}_i^\top\). Assuming all keys are normalized to unit length, when we try to retrieve a value associated with a specific key \(k_j\), we get:</p> \[\begin{aligned} \mathbf{S}\mathbf{k}_j &amp;= \sum \mathbf{v}_i (\mathbf{k}_i^\top \mathbf{k}_j) \\ &amp;= \mathbf{v}_j + \underbrace{\sum_{i\neq j} (\mathbf{k}_i^\top \mathbf{k}_j)\mathbf{v}_i}_{\text{retrieval error}} \end{aligned}\] <p>To minimize the retrieval error term, we need \(\mathbf{k}_i^\top \mathbf{k}_j = 0\) for all \(i\neq j\) - in other words, all keys should be <strong>orthogonal</strong> to each other. However, this reveals a fundamental limitation: in a \(d\)-dimensional space, you can only have at most \(d\) orthogonal vectors. This explains why increasing head dimension helps (For example, Sun et al.<d-cite key="sun2023retentive"></d-cite> have demonstrated the necessity of increasing head dimensions to enhance model performance) - it provides more ‚Äúroom‚Äù in the vector space for storing distinct key-value pairs!</p> <p>This theoretical limitation manifests in practice: vanilla linear attention has underperformed compared to softmax attention (by a large marghin) in language modeling. The primary cause is memory ‚Äúoverload‚Äù: in this key-value associative memory system, we can only add new key-value associations without the ability to erase existing information. As sequences grow longer, this leads to accumulating ‚Äúretrieval errors‚Äù that degrade performance. Indeed, as noted by David Eagleman in his book ‚ÄúLivewired: The Inside Story of the Ever-Changing Brain‚Äù,</p> <blockquote> <p>‚ÄúThe enemy of memory is not time; it‚Äôs other memories.‚Äù</p> </blockquote> <p>(Thanks to Kazuki Irie for the reference!). Recent advances in gated variants of linear attention (such as GLA<d-cite key="yang_gated_2023"></d-cite> and Mamba<d-cite key="Gu2023MambaLS"></d-cite>) have significantly narrowed the performance gap with standard attention in language modeling tasks by incorporating a <strong>forgetting mechanism</strong>. However, these models still face fundamental challenges with in-context retrieval and exact copying capabilities‚Äîlimitations that have been both empirically observed and theoretically proven in recent work<d-cite key="zoology"></d-cite><d-cite key="arora_simple_2024"></d-cite><d-cite key="jelassi_repeat_2024"></d-cite>.</p> <details> <summary>Click here to learn more about gated variants of linear attention</summary> Given the close relationship between linear attention and RNN, it is no wonder that researchers want to enhance linear attention with the (forgetting) gating mechanisms, which has been shown unreasonably effective in nonlinear RNN<d-cite key="unreasonable-forget-gate"></d-cite> and linear RNN<d-cite key="HGRN"></d-cite>: <p> </p> <p> \[\mathbf{S}_t = \mathbf{G}_t \odot \mathbf{S}_{t-1} + \mathbf{v}_t\mathbf{k}_t^\top\] </p> <p> with different structured parameterization for \(\mathbf{G}_t \in \mathbb{R}^{d\times d}\) for parameter efficiency, often with outer product structure. Different models have proposed various ways to structure this gating matrix: </p> <p> For Decaying Fast weight<d-cite key="mao_fine-tuning_2022"></d-cite>: \[\mathbf{G}_t = \mathbf{\beta_t} \mathbf{\alpha_t}^\top\] </p> <p> For GLA<d-cite key="yang_gated_2023"></d-cite>: \[\mathbf{G}_t = \mathbf{1} \mathbf{\alpha_t}^\top\] </p> <p> For Mamba1<d-cite key="Gu2023MambaLS"></d-cite>: \[\mathbf{G}_t = \exp(-(\mathbf{\Delta_t} \mathbf{1}^\top) \odot \exp(A))\] </p> <p> For Mamba2<d-cite key="mamba2"></d-cite>: \[\mathbf{G}_t = \gamma_t \mathbf{1}\mathbf{1}^\top\] </p> <p> Cf. Table 1 of GLA<d-cite key="yang_gated_2023"></d-cite> for a summarization. </p> </details> <h2 id="deltanet-linear-attention-with-delta-rule">DeltaNet: Linear Attention with Delta Rule</h2> <h3 id="what-is-delta-rule">What is Delta Rule?</h3> <p>The Delta Rule<d-cite key="widrow_adaptive_1988"></d-cite> is a fundamental error-correction learning principle in neural networks. Its fcore idea is beautifully simple: adjust the model‚Äôs parameters based on the difference (delta) between what we want (target) and what we actually get (prediction).</p> <p>To understand this intuitively, imagine teaching a child to aim at a target. If they shoot too far to the left, you‚Äôd tell them to adjust right; too far right, adjust left. The size of the adjustment depends on how far they missed - a concept directly reflected in the Delta Rule.</p> <details> <summary>Click to expand Delta Rule code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">delta_rule</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Simple delta rule implementation
    x: input features (N samples by D features)
    y: target values (N samples)
    </span><span class="sh">"""</span>
    <span class="c1"># Initialize weights
</span>    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Train
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
            <span class="c1"># Forward pass
</span>            <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span>
            
            <span class="c1"># Compute error
</span>            <span class="n">error</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred</span>
            
            <span class="c1"># Update weights
</span>            <span class="n">w</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
    <span class="k">return</span> <span class="n">w</span>

<span class="c1"># Example usage
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="c1"># Generate toy data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 100 samples, 3 features
</span>    <span class="n">true_w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_w</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    
    <span class="c1"># Train
</span>    <span class="n">w</span> <span class="o">=</span> <span class="nf">delta_rule</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">True weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">true_w</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Learned weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></code></pre></figure> </details> <h3 id="what-is-deltanet">What is DeltaNet?</h3> <p>DeltaNet<d-cite key="schlag_linear_2021"></d-cite> applies this error-correction principle to linear attention. Instead of simply accumulating key-value outer product, it updates its state based on prediction errors:</p> \[\begin{align*} \mathbf{S}_{t} &amp;= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \end{align*}\] <p>The parallel to the Delta Rule becomes clear when we break down the components:</p> <ul> <li>\(\beta_t \in \mathbb{R}\) acts as the learning rate</li> <li>\(\mathbf{k}_t \in \mathbb{R}^d\) is the input data</li> <li>\(\mathbf{v}_t \in \mathbb{R}^d\) is the target</li> <li>\(\mathbf{S}_{t-1} \mathbf{k}_t \in \mathbb{R}^d\) is our current prediction</li> </ul> <p>We will revisit this form later, showing how it can emerge naturally from a single gradient descent step on a (online) loss function.</p> <p>There‚Äôs another intuitive way to understand this update rule. Think of \(\mathbf{S}_{t-1}\mathbf{k}_t\) as retrieving the ‚Äúold value‚Äù associated with the current key \(\mathbf{k}_t\) from memory. When we encounter a newly associated value \(\mathbf{v}_t\) for the same key, rather than blindly overwriting, we make a careful update:</p> \[\begin{align*} \mathbf{v}_t^{\text{new}} &amp;= (1-\beta_t) \mathbf{v}_t^{\text{old}} + \beta_t \mathbf{v}_t, \\ \mathbf{S}_t &amp;= \mathbf{S}_{t-1} - \underbrace{\mathbf{v}_t^{\text{old}} \mathbf{k}_t^\top}_{\text{erase}} + \underbrace{\mathbf{v}_t^{\text{new}} \mathbf{k}_t^\top}_{\text{write}} \end{align*}\] <p>where \(\mathbf{v}_t^{\text{new}}\) is a learned combination of the old and current values, controlled by a dynamic \(\beta_t \in (0,1)\): when \(\beta_t=0\), the memory content remains intact, and when \(\beta_t=1\), we completely replace the old associated value with the new one.</p> <h3 id="deltanet-as-a-strong-in-context-learning-rnn">DeltaNet as a Strong In-context Learning RNN</h3> <p>MQAR (Multi-Query Associative Recall)<d-cite key="zoology"></d-cite> is a recent popular synthetic benchmark aimed at measuring the in-context associative recall ability for subquadratic models.</p> <p>The MQAR task works as follows: Each letter is associated with a number, and the model is asked to correctly recall the number associated with each letter in a query sequence.</p> <p>For example, given the input:</p> <p><code class="language-plaintext highlighter-rouge">A 4 B 3 C 6 F 1 E 2 ‚Üí A ? C ? F ? E ? B ?</code></p> <p>The format consists of:</p> <ol> <li>Key-Value pairs (before the arrow): Letters paired with their corresponding numbers</li> <li>Query sequence (after the arrow): Letters whose associated numbers need to be recalled</li> </ol> <p>The correct output for this example would be:</p> <p><code class="language-plaintext highlighter-rouge">4, 6, 1, 2, 3</code></p> <p>While conventional gated convolution and recurrent models generally underperform in this task, in our experiments, we show that DeltaNet <d-footnote>Interestingly, DeltaNet was initially designed to improve associative recall performance but remained largely overlooked until this work.</d-footnote> demonstrates notably strong performance:</p> <div class="row justify-content-center"> <div class="col-6"> <img class="img-fluid" style="background-color: white; padding: 20px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" src="/assets/img/blog/deltanet/mqar-1.png"/> </div> </div> <div class="caption"> The hardest setting from the original Zoology paper </div> <p>This initial success was particularly exciting‚Äîachieving perfect performance on MQAR exceeded our expectations. What makes this result especially promising is that MQAR performance strongly correlates with ‚ÄúAssociative-Recall-Hit‚Äù in real-world language modeling tasks<d-cite key="zoology"></d-cite>. Associative recall failures are a primary source of errors in subquadratic models and largely account for their perplexity gap relative to softmax attention. Thus, DeltaNet‚Äôs perfect MQAR performance suggested significant potential.</p> <p>We‚Äôve also conducted experiments on MAD<d-cite key="poli_mechanistic_2024"></d-cite>, another more comprehensive benchmark than MQAR that is also motivated to test new architecture‚Äôs capacities, and the results are summarized below:</p> <table> <thead> <tr> <th>Model</th> <th>Compress</th> <th>Fuzzy Recall</th> <th>In-Context Recall</th> <th>Memorize</th> <th>Noisy Recall</th> <th>Selective Copy</th> <th>Average</th> </tr> </thead> <tbody> <tr> <td>Transformer</td> <td>51.6</td> <td>29.8</td> <td>94.1</td> <td>85.2</td> <td>86.8</td> <td>99.6</td> <td>74.5</td> </tr> <tr> <td>Hyena</td> <td>45.2</td> <td>7.9</td> <td>81.7</td> <td>89.5</td> <td>78.8</td> <td>93.1</td> <td>66.0</td> </tr> <tr> <td>Multihead Hyena</td> <td>44.8</td> <td>14.4</td> <td>99.0</td> <td>89.4</td> <td>98.6</td> <td>93.0</td> <td>73.2</td> </tr> <tr> <td>Mamba</td> <td>52.7</td> <td>6.7</td> <td>90.4</td> <td>89.5</td> <td>90.1</td> <td>86.3</td> <td>69.3</td> </tr> <tr> <td>GLA</td> <td>38.8</td> <td>6.9</td> <td>80.8</td> <td>63.3</td> <td>81.6</td> <td>88.6</td> <td>60.0</td> </tr> <tr> <td>DeltaNet</td> <td>42.2</td> <td>35.7</td> <td>100</td> <td>52.8</td> <td>100</td> <td>100</td> <td>71.8</td> </tr> </tbody> </table> <p>where DeltaNet demonstrates its strong in-context recall capacities. These synthetic tasks are inexpensive to run and offer clear evidence that DeltaNet is likely to perform well at scale. This motivated us to focus on developing DeltaNet‚Äôs training algorithm and kernel implementation‚Äîafter all, scaling up an arbitrary architecture without demonstrating its potential would risk wasting significant time and resources.</p> <p>In the next post, we‚Äôll explore a beautiful algorithm that parallelizes DeltaNet across sequence length. But first, let‚Äôs build some intuition about why DeltaNet is particularly well-suited for in-context retrieval tasks.</p> <h3 id="why-is-deltanet-superior-at-in-context-retrieval-compared-to-linear-attention">Why is DeltaNet Superior at In-context Retrieval Compared to Linear Attention?</h3> <p>DeltaNet‚Äôs update rule can be derived by sequentially minimizing the mean squared error (MSE) between the desired output and the predicted output at each time step \(t\) using gradient descent: <d-footnote>This formulation reveals an interesting connection to Test-Time-Training (TTT) <d-cite key="sun-2024-learning"></d-cite>: DeltaNet becomes mathematically equivalent to TTT-linear under two specific conditions: (1) when nonlinear components such as layer normalization are removed, and (2) when the mini-batch size in TTT is set to one.</d-footnote></p> \[\mathcal{L}_t(\mathbf{S}) = \frac{1}{2}\|\mathbf{S} \mathbf{k}_t - \mathbf{v}_t\|^2\] <p>Applying gradient descent to minimize this MSE loss gives:</p> \[\begin{aligned} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} - \eta_t \nabla \mathcal{L}_t(\mathbf{S}_{t-1}) \\ &amp;= \mathbf{S}_{t-1} - \eta_t \left(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t\right) \mathbf{k}_t^\top \end{aligned}\] <p>When the learning rate \(\eta_t\) is set to \(\beta_t\), this results in DeltaNet‚Äôs update rule.</p> <p>In contrast, vanilla linear attention employs a linear loss function:</p> \[\mathcal{L}^\prime_t(\mathbf{S}) = -\langle \mathbf{S} \mathbf{k}_t, \mathbf{v}_t \rangle\] <p>The corresponding update rule for linear attention is:</p> \[\begin{aligned} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} - \eta_t \nabla \mathcal{L}_t^\prime(\mathbf{S}_{t-1}) \\ &amp;= \mathbf{S}_{t-1} + \eta_t \mathbf{v}_t \mathbf{k}_t^\top \end{aligned}\] <p>By setting \(\eta_t = 1\), the standard linear attention update is recovered.</p> <p>Thus, DeltaNet‚Äôs superior performance in in-context retrieval becomes evident‚Äîit minimizes MSE at each step, making it ideal for tasks like associative recall where reducing large errors is crucial for accurate retrieval.</p>]]></content><author><name>Songlin Yang</name></author><summary type="html"><![CDATA[A gentle and comprehensive introduction to the DeltaNet]]></summary></entry><entry><title type="html">DeltaNet Explained (Part II)</title><link href="https://sustcsonglin.github.io/blog/2024/deltanet-2/" rel="alternate" type="text/html" title="DeltaNet Explained (Part II)"/><published>2024-12-03T22:25:00+00:00</published><updated>2024-12-03T22:25:00+00:00</updated><id>https://sustcsonglin.github.io/blog/2024/deltanet-2</id><content type="html" xml:base="https://sustcsonglin.github.io/blog/2024/deltanet-2/"><![CDATA[<p><strong>This blog post series accompanies our NeurIPS ‚Äò24 paper - <a href="https://arxiv.org/abs/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></strong> (w/ <a href="https://berlino.github.io/">Bailin Wang</a>, <a href="https://yzhang.site/">Yu Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/yikang-shen/">Yikang Shen</a> and <a href="https://people.csail.mit.edu/yoonkim/">Yoon Kim</a>). <strong>You can find the implementation <a href="https://github.com/sustcsonglin/flash-linear-attention/blob/main/fla/layers/delta_net.py">here</a> and the presentation slides <a href="https://people.csail.mit.edu/yoonkim/data/efficient_architectures_talk.pdf">here</a>.</strong></p> <ol> <li><a href="/blog/2024/deltanet-1/">Part I - The Model</a></li> <li><a href="#">Part II - The Algorithm</a></li> <li><a href="/blog/2024/deltanet-3/">Part III - The Neural Architecture</a></li> </ol> <h2 id="parallel-scan-for-deltanet-a-failed-attempt">Parallel Scan for DeltaNet: A Failed Attempt</h2> <h3 id="from-delta-updates-to-matrix-multiplication-form">From Delta Updates to Matrix Multiplication Form</h3> <p>Let‚Äôs start with DeltaNet‚Äôs original state update equation:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top\] <p>To transform this into a matrix multiplication form, let‚Äôs expand the equation step by step:</p> \[\begin{align*} \mathbf{S}_{t} &amp;= \mathbf{S}_{t-1} - \beta_t(\mathbf{S}_{t-1} \mathbf{k}_t - \mathbf{v}_t)\mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} - \beta_t \mathbf{S}_{t-1} \mathbf{k}_t \mathbf{k}_t^\top + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \end{align*}\] <p>For simplicity, let‚Äôs denote:</p> <ul> <li>\(\mathbf{M}_t = \mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal\) as our transition matrix</li> <li>\(\mathbf{X}_t = \beta_t \mathbf{v}_t \mathbf{k}_t^\top\) as our update term</li> </ul> <p>Then our update becomes:</p> \[\mathbf{S}_{t} = \mathbf{S}_{t-1}\mathbf{M}_t + \mathbf{X}_t \in \mathbb{R}^{d\times d}\] <h3 id="defining-the-associative-operator">Defining the Associative Operator</h3> <p>This form matches exactly with the first-order recurrence shown in equation (1.5) from <em>Prefix Sums and Their Applications</em><d-cite key="Blelloch1990PrefixSA"></d-cite>, where matrix multiplication (‚äó) and matrix addition (‚äï) serve as our binary operators. Both operators satisfy the required properties:</p> <ol> <li>Matrix addition is associative: \((A + B) + C = A + (B + C)\)</li> <li>Matrix multiplication is associative: \((AB)C = A(BC)\)</li> <li>Matrix multiplication distributes over addition: \(A(B + C) = AB + AC\)</li> </ol> <p>Following the framework, we define our state pairs as:</p> \[c_t = [\mathbf{M}_t, \mathbf{X}_t] = [\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\intercal, \beta_t \mathbf{v}_t \mathbf{k}_t^\top]\] <p>And our associative operator ‚Ä¢ that combines these pairs:</p> \[c_i \bullet c_j = [\mathbf{M}_i\mathbf{M}_j, \mathbf{M}_j\mathbf{X}_i + \mathbf{X}_j]\] <p>This operator definition preserves the temporal dependencies of our updates - when we combine two steps, the earlier update term \(\mathbf{X}_i\) must be transformed by the later transition matrix \(\mathbf{M}_j\), while the later update term \(\mathbf{X}_j\) remains unchanged.</p> <h3 id="parallel-scan-for-deltanet">Parallel Scan for DeltaNet</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/scan.png" alt="Á§∫‰æãÂõæÁâá" style="width: 99%"/> </div> </div> <p>With this associative operator, we can use parallel scan to compute all states in parallel. The algorithm works in two phases:</p> <h5 id="sweep-down-phase">Sweep-Down Phase</h5> <p>First, we compute partial results in parallel by combining adjacent pairs:</p> <p>For steps 0 and 1, we compute:</p> \[c_1 = c_0 \bullet c_1 = [\mathbf{M}_0\mathbf{M}_1, \mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1]\] <p>Similarly for steps 2 and 3:</p> \[c_3 = c_2 \bullet c_3 = [\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <p>Then combine these results:</p> \[c_{1:3} = c_{1} \bullet c_{3} = [\mathbf{M}_0\mathbf{M}_1\mathbf{M}_2\mathbf{M}_3, \mathbf{M}_2\mathbf{M}_3(\mathbf{M}_1\mathbf{X}_0 + \mathbf{X}_1) + \mathbf{M}_3\mathbf{X}_2 + \mathbf{X}_3]\] <h5 id="sweep-up-phase">Sweep-Up Phase</h5> <p>In this phase, we use our partial results to compute intermediate states:</p> \[c_2 = c_1 \bullet c_2 = [\mathbf{M}_1\mathbf{M}_2, \mathbf{M}_2\mathbf{X}_1 + \mathbf{X}_2]\] <p>This parallelization transforms DeltaNet‚Äôs sequential state updates into an efficient parallel computation, reducing the sequential dependency chain from \(\mathbf{O}(L)\) to \(\mathcal{O}(\log L)\) steps while maintaining mathematical equivalence.</p> <h3 id="whats-wrong-with-parallel-scan-for-deltanet">What‚Äôs Wrong with Parallel Scan for DeltaNet?</h3> <p>Despite parallelizability, parallel scan for DeltaNet faces two major challenges: computational complexity and memory requirements.</p> <p>The first issue lies in the <strong>time complexity</strong>. For DeltaNet, parallel scan yields \(\mathcal{O}(L\log L d^3)\) complexity due to the cubic cost of matrix multiplication when treating \(\mathbf{M}_t\) as dense matrices. At first glance, we might think we can leverage the identity-plus-low-rank structure of \(\mathbf{M}_t\) for acceleration. Let‚Äôs work through this carefully.</p> <p>When multiplying two adjacent matrices, we get:</p> \[\begin{align*} (\mathbf{I}-\beta_0 \mathbf{k}_0 \mathbf{k}_0^\top)(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) &amp;= \mathbf{I}(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) \\ &amp;= (\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top) - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 \mathbf{k}_0^\top \mathbf{k}_1 \mathbf{k}_1^\top \\ &amp;= \mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top \end{align*}\] <p>This computation reduces the complexity from \(\mathcal{O}(d^3)\) to \(\mathcal{O}(d^2)\) by leveraging the identity-plus-low-rank structure - we only need to compute vector inner products \((\mathbf{k}_0^\top \mathbf{k}_1)\) and outer products between vectors. Similarly for the next pair:</p> \[\begin{align*} (\mathbf{I}-\beta_2 \mathbf{k}_2 \mathbf{k}_2^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top) &amp;= \mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top \end{align*}\] <p>When we try to combine these results to compute larger spans like \(c_{1:4}\), the multiplication becomes increasingly complex. We need to multiply:</p> \[(\mathbf{I} - \beta_1 \mathbf{k}_1 \mathbf{k}_1^\top - \beta_0 \mathbf{k}_0 \mathbf{k}_0^\top + \beta_0\beta_1 \mathbf{k}_0 (\mathbf{k}_0^\top \mathbf{k}_1) \mathbf{k}_1^\top)(\mathbf{I} - \beta_3 \mathbf{k}_3 \mathbf{k}_3^\top - \beta_2 \mathbf{k}_2 \mathbf{k}_2^\top + \beta_2\beta_3 \mathbf{k}_2 (\mathbf{k}_2^\top \mathbf{k}_3) \mathbf{k}_3^\top)\] <p>Each term in the first bracket must multiply with each term in the second bracket, leading to a quadratic growth in the number of terms. This quickly becomes unmanageable due to the combinatorial explosion of terms. This suggests we might be better off just treating it as dense matrix multiplication.</p> <p>The second major issue is <strong>space complexity</strong>. Parallel scan requires materializing all intermediate d√ód matrices at each step to high-bandwidth memory (HBM). For linear RNNs with matrix-valued states, this materialization becomes prohibitively expensive (\(\mathcal{O}(Ld^2)\)). While recurrent computation can avoid such materialization, parallel scan offers no apparent workaround unless all states fit into SRAM, as implemented in Mamba‚Äôs hardware-aware selective scan algorithm that eliminates the need for materialization. However, this approach imposes limitations on state size - too large a state leads to out-of-shared-memory issues. Given that I/O costs dominate this computation, parallel scan may become undesirable in practice. As noted in recent discussions:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Dear parallel-scan people.<br/><br/>Q(t) and K(t) are 1xD and V(t) is 1xC, with D and T large, and I want to compute<br/><br/>Y(t) = Q(t)M(t)<br/><br/>with<br/><br/>M(0)=0<br/>M(t+1) = M(t) + K(t)^T V(t)<br/><br/>A naive non-parallel scan approach is O(T) in time but I do not have to store any DxD<br/><br/>1/2</p>&mdash; Fran√ßois Fleuret (@francoisfleuret) <a href="https://twitter.com/francoisfleuret/status/1793016689589625263?ref_src=twsrc%5Etfw">May 21, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">The key idea is to only materialize chunk-level hidden states, using matmuls to calculate outputs based on the query, key, and value matrices and chunk-level hidden states. This method avoids materializing the hidden state for every single timestep.</p>&mdash; Songlin Yang üöÇ NeurIPS &#39;24 (@SonglinYang4) <a href="https://twitter.com/SonglinYang4/status/1793029555277697379?ref_src=twsrc%5Etfw">May 21, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>Here I previously discussed the chunkwise algorithm - another type of associative scan that offers improved memory efficiency and better utilization of tensor cores by enabling more matrix multiplication operations (for a detailed analysis, see <d-cite key="yang_gated_2023"></d-cite>). Given these advantages, developing a chunkwise training algorithm for DeltaNet that maintains quadratic complexity with respect to \(d\) while preserving memory efficiency would be highly valuable.</p> <h2 id="a-chunkwise-algorithm-for-deltanet">A Chunkwise Algorithm for DeltaNet</h2> <h3 id="chunkwise-parallel-form-for-linear-attention">Chunkwise Parallel Form for Linear Attention</h3> <p>Linear attention‚Äôs efficiency stems from its ability to maintain a compact representation of the state using vectors rather than materializing full matrices. This is possible because a sum of outer products can be rewritten as matrix multiplication:</p> \[\begin{align*} \sum_{i=1}^t \mathbf{v}_i \mathbf{k}_i^\top &amp;= \mathbf{V}_t\mathbf{K}_t^\top \\ \text{where } \mathbf{V}_t &amp;= [\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_t] \\ \mathbf{K}_t &amp;= [\mathbf{k}_1, \mathbf{k}_2, ..., \mathbf{k}_t] \end{align*}\] <p>This matrix multiplication form is highly optimized on modern GPUs with tensor cores. Leveraging this property, instead of storing all intermediate hidden states, we can store states only at regular intervals of size \(C\) as checkpoints. This gives us states \(\mathbf{S}_{0}, \mathbf{S}_{C}, \mathbf{S}_{2C}, ..., \mathbf{S}_{(n-1)C}\) where \(n = \lceil L/C \rceil\).</p> <p>Denoting \(\mathbf{S}_{[i]} := \mathbf{S}_{iC} \in \mathbb{R}^{d \times d}\); \(\square_{[i]} = \square_{iC+1:(i+1)C} \in \mathbb{R}^{C \times d}\) for \(\square \in \{\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}\}\); \(\square_{[i]}^r = \square_{iC+j}\) for \(\square \in \{\mathbf{q}, \mathbf{k}, \mathbf{v}, \mathbf{o}, \mathbf{S}\}\). For any position j within chunk i, we can compute:</p> \[\begin{align*} \mathbf{S}_{[i]}^r &amp;= \mathbf{S}_{[i]} + \sum_{t=1}^{r} \mathbf{v}_{[i]}^t \mathbf{k}_{[i]}^{t\top} \\ \mathbf{o}_{[i]}^r &amp;= \mathbf{S}_{[i]}^r \mathbf{q}_{[i]}^r = \mathbf{S}_{[i]}\mathbf{q}_{[i]}^r + \sum_{t=1}^{r} \mathbf{v}_{[i]}^t (\mathbf{k}^{t\top}_{[i]} \mathbf{q}_{[i]}^r) \end{align*}\] <p>and in matrix form,</p> \[\begin{align*} \mathbf{S}_{[t+1]} &amp;= \mathbf{S}_{[t]} + \mathbf{V}_{[t]}^\top \mathbf{K}_{[t]} &amp;&amp; \in \mathbb{R}^{d\times d} \\ \mathbf{O}_{[t]} &amp;= \mathbf{Q}_{[t]} \mathbf{S}_{[t]}^\top + (\mathbf{Q}_{[t]}\mathbf{K}_{[t]}^\top \odot \mathbf{M}) \mathbf{V}_{[t]} &amp;&amp; \in \mathbb{R}^{C\times d} \end{align*}\] <p>This chunkwise formulation enables efficient hardware utilization by leveraging tensor cores when the chunk size C is a multiple of 16, as implemented in our open-source library <strong>flash-linear-attention</strong><d-cite key="yang_fla_2024"></d-cite>.</p> <h3 id="wy-representation-for-deltanet">WY representation for DeltaNet</h3> <p>However, as we saw in our failed attempt above, the cumulative product of DeltaNet‚Äôs transition matrices seemed to resist such compact representation, apparently requiring us to store numerous intermediate results. Fortunately, there‚Äôs a solution: DeltaNet‚Äôs transition matrices closely resemble Householder matrices (when \(\beta_t\)=2), and there exists an elegant compact representation for their cumulative product.</p> <p>This representation, known as the WY representation, was introduced in a seminal 1985 paper<d-cite key="bischof_wy_1985"></d-cite>. Using DeltaNet‚Äôs notation, the cumulative product can be written as:</p> \[\prod_{i=1}^{t} (\mathbf{I} - \beta_i \mathbf{k}_i \mathbf{k}_i^\top) = \mathbf{I} - \sum_{i=1}^t \mathbf{w}_i\mathbf{k}_i^\top\] <p>We can prove this by mathematical induction. Let‚Äôs define \(\mathbf{P}_n = \prod_{t=1}^n(\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\top)\). For n=1, the equation clearly holds. Assuming it holds for n-1, we can prove it for n:</p> \[\begin{align*} \mathbf{P}_n &amp;= \mathbf{P}_{n-1} (\mathbf{I} - \beta_n \mathbf{k}_n \mathbf{k}_n^\top) \\ &amp;= (\mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top)(\mathbf{I} - \beta_n \mathbf{k}_n \mathbf{k}_n^\top) \\ &amp;= \mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top - \beta_n \mathbf{k}_n \mathbf{k}_n^\top + (\sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top) \beta_n \mathbf{k}_n \mathbf{k}_n^\top \\ &amp;= \mathbf{I} - \sum_{t=1}^{n-1} \mathbf{w}_t \mathbf{k}_t^\top - \underbrace{\left(\beta_n \mathbf{k}_n - \beta_n \sum_{t=1}^{n-1} \left(\mathbf{w}_t (\mathbf{k}_t^\top\mathbf{k}_n)\right) \right)}_{\mathbf{w}_n}\mathbf{k}_n^\top \\ &amp;= \mathbf{I} - \sum_{t=1}^n \mathbf{w}_t\mathbf{k}_t^\top \end{align*}\] <p>This proof not only establishes the correctness of the representation but also provides a constructive way to compute the \(\mathbf{w}\) vectors!</p> <p>Similarly, we can show \(\mathbf{S}_n = \sum_{t=1}^{n} \mathbf{u}_t \mathbf{k}_n^\top\) by induction:</p> \[\begin{align*} \mathbf{S}_n &amp;= \mathbf{S}_{n-1} (\mathbf{I} - \beta_n \mathbf{k}_n\mathbf{k}_n^\top) + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \left(\sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top\right) (\mathbf{I} - \beta_n \mathbf{k}_n\mathbf{k}_n^\top) + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top - \left(\sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top\right) \beta_n \mathbf{k}_n \mathbf{k}_n^\top + \beta_n \mathbf{v}_n \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n-1} \mathbf{u}_{t}\mathbf{k}_{t}^\top + \underbrace{\left(\beta_n \mathbf{v}_n - \beta_n\sum_{t=1}^{n-1} \mathbf{u}_t \left(\mathbf{k}_t^\top \mathbf{k}_n \right) \right)}_{\mathbf{u}_n} \mathbf{k}_n^\top \\ &amp;= \sum_{t=1}^{n} \mathbf{u}_t \mathbf{k}_n^\top \end{align*}\] <p>Looking at this sum-of-outer-products structure, we can see it closely resembles linear attention‚Äôs update form. This similarity suggests a path toward developing a novel parallel algorithm!</p> <h3 id="chunkwise-parallel-form-for-deltanet">Chunkwise Parallel Form for DeltaNet</h3> <p>First, let‚Äôs unroll the recurrence of DeltaNet:</p> \[\begin{align*} \mathbf{S}_t &amp;= \mathbf{S}_{t-1} (\mathbf{I} - \beta_t \mathbf{k}_t \mathbf{k}_t^\top) + \beta_t \mathbf{v}_t \mathbf{k}_t^\top \\ &amp;= \sum_{i=1}^t \beta_i (\mathbf{v}_i \mathbf{k}_i^\top) \left(\prod_{j=i+1}^t (\mathbf{I} - \beta_j \mathbf{k}_j \mathbf{k}_j^\top)\right) \end{align*}\] <p>Similar to linear attention, we can use checkpointing to store states at regular intervals of size C. For any position r within chunk i, we have:</p> \[\begin{align*} \mathbf{S}_{[i]}^r &amp;= \mathbf{S}_{[i]} \underbrace{\prod_{t=1}^{r} (\mathbf{I}-\beta_{[i]}^t\mathbf{k}_{[i]}^t\mathbf{k}_{[i]}^{t\top})}_{\text{chunk-local cumprod: } \mathbf{P}_{[i]}^r} + \underbrace{\sum_{t=1}^{r} (\beta_{[i]}^t \mathbf{v}_{[i]}^t \mathbf{k}_{[i]}^{t\top} \prod_{s=t+1}^{r} (\mathbf{I}-\beta_{[i]}^s\mathbf{k}_{[i]}^s\mathbf{k}_{[i]}^{s\top}))}_{\text{chunk-local state or cumprodsum: }\mathbf{H}_{[i]}^r} \\ &amp;= \mathbf{S}_{[i]} (\mathbf{I} - \sum_{t=1}^r\mathbf{w}_{[i]}^t\mathbf{k}_{[i]}^{t\top}) + \sum_{t=1}^r \mathbf{u}_{[i]}^t \mathbf{k}_{[i]}^{t\top}\\ \end{align*}\] <p>where \(\mathbf{w}_{[i]}^t\) and \(\mathbf{u}_{[i]}^t\) are computed using the WY representation, but starting from the first position of each chunk rather than the beginning of the sequence, enabling parallel computation across chunks.</p> \[\mathbf{w}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{k}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{w}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] \[\mathbf{u}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{v}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] <p>And for output computation,</p> \[\begin{align*} \mathbf{o}_{[i]}^r &amp;= \mathbf{S}_{[i]}^r \mathbf{q}_{[i]}^r \\ &amp;= \mathbf{S}_{[i]} \mathbf{q}_{[i]}^r - \sum_{t=1}^r \mathbf{S}_{[i]}\mathbf{w}_{[i]}^t(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) + \sum_{t=1}^r \mathbf{u}_{[i]}^t(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) \\ &amp;= \mathbf{S}_{[i]} \mathbf{q}_{[i]}^r + \sum_{t=1}^r (\mathbf{u}_{[i]}^t - \mathbf{S}_{[i]}\mathbf{w}_{[i]}^t)(\mathbf{k}_{[i]}^{t\top}\mathbf{q}_{[i]}^r) \end{align*}\] <p>Together, in matrix-multiplication form,</p> \[\begin{align*} \mathbf{S}_{[i+1]} &amp;= \mathbf{S}_{[t]} (\mathbf{I}-\mathbf{W}_{[i]}^\top \mathbf{K}_{[i]}) + \mathbf{U}_{[i]}^\top \mathbf{K}_{[i]} \\ &amp;= \mathbf{S}_{[i]} + \left(\mathbf{U}_{[i]} - \mathbf{W}_{[i]}\mathbf{S}_{[i]}^\top\right)^\top \mathbf{K}_{[i]} &amp;&amp; \in \mathbb{R}^{d\times d} \\ \mathbf{O}_{[i]} &amp;= \mathbf{Q}_{[i]} \mathbf{S}_{[i]}^\top + (\mathbf{Q}_{[i]} \mathbf{K}_{[i]}^\top \odot \mathbf{M}) \left(\mathbf{U}_{[i]} - \mathbf{W}_{[i]} \mathbf{S}_{[i]}^\top\right) &amp;&amp; \in \mathbb{R}^{C\times d} \end{align*}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/delta-chunk.png" alt="Á§∫‰æãÂõæÁâá" style="width: 99%"/> </div> </div> <h3 id="ut-transform-through-the-lens-of-graph-theory">UT Transform Through the Lens of Graph Theory</h3> <p>The chunkwise parallel form transforms most of DeltaNet‚Äôs operations into efficient matrix multiplications similar to linear attention, making them well-suited for modern hardware with tensor cores. However, there‚Äôs a key computational bottleneck: the recursive construction of update vectors \(\mathbf{U}_{[i]}\) and \(\mathbf{W}_{[i]}\). This sets up the motivation for why we need the UT transform<d-cite key="Joffrain2006AccumulatingHT"></d-cite> - we want to restructure this recursive computation into a form that, like the rest of our operations, can take advantage of efficient matrix multiplication on modern hardware. But rather than directly introducing the UT transform, let‚Äôs build intuition through the lens of graph theory.</p> <p>First, recall how adjacency matrices work in graph theory. For a weighted directed graph, the adjacency matrix \(\mathbf{A}\) captures direct connections between nodes - entry \(\mathbf{A}[i,j]\) represents the weight of the edge from node \(j\) to node \(i\). Now, consider what happens when we compute \((\mathbf{I} - \mathbf{A})^{-1}\). This inverse has a beautiful interpretation: each entry \([i,j]\) gives us the sum of weights of all possible paths from \(j\) to \(i\), including both direct connections and indirect paths through other nodes.</p> <p>Now look at recursive update equations:</p> \[\mathbf{w}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{k}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] \[\mathbf{u}_{[t]}^r = \beta_{[t]}^r \left(\mathbf{v}_{[t]}^r - \sum_{i=1}^{r-1} \mathbf{u}_{[t]}^i (\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r \right)\] <p>When position \(r\) updates its state, it‚Äôs influenced by all previous positions \(i &lt; r\) through the term \((\mathbf{k}_{[t]}^i)^\top\mathbf{k}_{[t]}^r\). This is exactly like a weighted edge in a graph! The weight of influence from position \(i\) to position \(r\) is determined by how similar their key vectors are, scaled by the learning rate \(\beta_{[t]}^r\).</p> <p>This insight leads us to define our ‚Äúadjacency matrix‚Äù:</p> \[\mathbf{A}_{[t]} = \operatorname{tril}(\operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{K}_{[t]} \mathbf{K}_{[t]}^\top,-1)\] <p>Each entry \([i,j]\) in this matrix represents how position \(i\) is directly influenced by position \(j\) through their key interaction \(\beta_i\mathbf{k}_i^\top\mathbf{k}_j\). The lower triangular structure (enforced by tril) ensures we only have edges from earlier positions to later ones - our graph is directed and acyclic.</p> <p>When we take \((\mathbf{I} - \mathbf{A}_{[t]})^{-1}\) in our transformation:</p> \[\mathbf{T}_{[t]} = \left(\mathbf{I} - \mathbf{A}_{[t]}\right)^{-1}\] <p>We‚Äôre doing exactly what the graph theory interpretation suggests - computing all possible paths of influence between positions. Just as \((\mathbf{I} - \mathbf{A})^{-1}\) in graph theory accumulates direct and indirect paths, our transformation matrix \(\mathbf{T}_{[t]}\) accumulates all ways that information can flow from each position to later positions through the chain of updates.</p> <p>This is why our transformation works - it‚Äôs not just a mathematical trick, but a way to efficiently compute all the cascading influences that would naturally arise from applying our updates sequentially. The graph theory connection gives us both theoretical insight (understanding why the transformation preserves equivalence) and practical benefits (efficient matrix inverse computation for lower triangle matrices through forward substitution due to the acyclic nature of our dependency graph).</p> <p>The final multiplication with keys and values:</p> \[\mathbf{W}_{[t]} = \mathbf{T}_{[t]} \operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{K}_{[t]}, \quad \mathbf{U}_{[t]}=\mathbf{T}_{[t]}\operatorname{diag}(\boldsymbol{\beta}_{[t]})\mathbf{V}_{[t]}\] <p>Then applies these accumulated influences to actually compute our updates, now in a form that can leverage efficient matrix multiplication on modern hardware.</p> <h3 id="speed-comparison">Speed comparison</h3> <p>We implemented both the recurrent and chunkwise parallel versions of DeltaNet using Triton. Our experiments compare their performance across different sequence lengths (\(L\)) and head dimensions (\(d_{\text{head}}\)), with a fixed model dimension \(d=2048\). To ensure fair comparison across configurations, we kept the total sequence elements constant at 16,384 by adjusting batch sizes accordingly.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/speedup.png" alt="Á§∫‰æãÂõæÁâá" style="width: 60%"/> </div> </div> <p>As we can see in the figure above, our chunkwise parallel approach consistently outperforms the recurrent baseline. More importantly, this performance advantage grows more pronounced under two key conditions: as sequences get longer and as head dimensions increase. To understand why, let‚Äôs examine two fundamental limitations of recurrent implementations that our approach addresses.</p> <p>The first limitation concerns parallelism strategy. Recurrent implementations process sequences step-by-step, relying primarily on two sources of parallelism to keep GPU cores busy: the batch dimension (processing multiple sequences simultaneously) and the head dimension (computing multiple attention heads in parallel). While this strategy worked well with moderate sequence lengths and larger batch sizes, it faces challenges in modern training scenarios. Today‚Äôs models increasingly work with longer sequences or larger model parameters, often necessitating smaller batch sizes for memory efficiency. This shift was notably highlighted in the FlashAttention2 paper<d-cite key="flashattention2"></d-cite>, which identified sequence-level parallelism as crucial for training. Without the ability to parallelize across the sequence dimension, recurrent implementations hit a fundamental bottleneck: when the product of batch size and number of attention heads is small, they can‚Äôt provide enough parallel work to keep modern GPUs fully utilized. This results in low occupancy of Streaming Multiprocessors (SMs) and suboptimal speed performance.</p> <p>The second limitation relates to hardware utilization. Modern GPUs include specialized tensor cores designed to accelerate matrix multiplication operations, offering up to 16x speedup for half-precision computations compared to other operations with equivalent FLOP counts. Recurrent implementations, despite requiring fewer total FLOPs, struggle to effectively leverage these hardware accelerators. This becomes particularly problematic with larger head dimensions, which are often necessary for tasks requiring substantial memory capacity (like in-context retrieval). Our chunkwise implementation, in contrast, restructures the computation to maximize use of tensor cores, achieving better real-world performance despite higher theoretical FLOP counts. This performance analysis illustrates a crucial principle in modern hardware-efficient deep learning: raw FLOP counts don‚Äôt always translate directly to wall-clock time. The ability to leverage specialized hardware accelerators and maintain high GPU utilization often matters more than theoretical operation counts. Our chunkwise implementation succeeds by aligning the computation with these hardware realities.</p> <p>Finally, we compare DeltaNet‚Äôs training throughput against other models at the 1.3B parameter scale.</p> <div class="row" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/blog/deltanet/throughputs.png" alt="Á§∫‰æãÂõæÁâá" style="width: 75%"/> </div> </div> <p>DeltaNet achieves competitive throughput, running only slightly slower than GLA (Gated Linear Attention). This small performance gap is a reasonable trade-off for DeltaNet‚Äôs more expressive transition matrices.</p>]]></content><author><name>Songlin Yang</name></author><summary type="html"><![CDATA[An algorithm that parallelizes DeltaNet computation across the sequence length dimension]]></summary></entry></feed>